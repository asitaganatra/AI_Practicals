{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "import string\n",
        "\n",
        "# Download required NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab') # Download the missing 'punkt_tab' data\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Perform complete text preprocessing including:\n",
        "    1. Tokenization\n",
        "    2. Lowercasing\n",
        "    3. Punctuation removal\n",
        "    4. Stopword removal\n",
        "    5. Stemming\n",
        "\n",
        "    Args:\n",
        "        text (str): Input text to preprocess\n",
        "\n",
        "    Returns:\n",
        "        list: List of processed tokens\n",
        "    \"\"\"\n",
        "    # 1. Tokenization\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # 2. Convert to lowercase\n",
        "    tokens = [word.lower() for word in tokens]\n",
        "\n",
        "    # 3. Remove punctuation\n",
        "    table = str.maketrans('', '', string.punctuation)\n",
        "    stripped = [w.translate(table) for w in tokens]\n",
        "    words = [word for word in stripped if word.isalpha()]\n",
        "\n",
        "    # 4. Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = [word for word in words if not word in stop_words]\n",
        "\n",
        "    # 5. Stemming\n",
        "    porter = PorterStemmer()\n",
        "    stemmed = [porter.stem(word) for word in words]\n",
        "\n",
        "    return stemmed\n",
        "\n",
        "# Example text (replace with your case study text)\n",
        "sample_text = \"\"\"\n",
        "Natural language processing (NLP) is a subfield of linguistics, computer science,\n",
        "and artificial intelligence concerned with the interactions between computers\n",
        "and human language. It focuses on how to program computers to process and\n",
        "analyze large amounts of natural language data.\n",
        "\"\"\"\n",
        "\n",
        "# Perform preprocessing\n",
        "processed_tokens = preprocess_text(sample_text)\n",
        "\n",
        "# Display results\n",
        "print(\"Original Text:\")\n",
        "print(sample_text)\n",
        "print(\"\\nPreprocessed Tokens:\")\n",
        "print(processed_tokens)\n",
        "print(\"\\nNumber of tokens:\", len(processed_tokens))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68Lv9hGQohD7",
        "outputId": "4a0e4060-76e6-46d7-adde-354e7ba26747"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text:\n",
            "\n",
            "Natural language processing (NLP) is a subfield of linguistics, computer science, \n",
            "and artificial intelligence concerned with the interactions between computers \n",
            "and human language. It focuses on how to program computers to process and \n",
            "analyze large amounts of natural language data.\n",
            "\n",
            "\n",
            "Preprocessed Tokens:\n",
            "['natur', 'languag', 'process', 'nlp', 'subfield', 'linguist', 'comput', 'scienc', 'artifici', 'intellig', 'concern', 'interact', 'comput', 'human', 'languag', 'focus', 'program', 'comput', 'process', 'analyz', 'larg', 'amount', 'natur', 'languag', 'data']\n",
            "\n",
            "Number of tokens: 25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ]
    }
  ]
}